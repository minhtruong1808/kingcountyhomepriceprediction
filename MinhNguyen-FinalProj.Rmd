---
title: "King County housing prediction"
author: "Minh Nguyen"
date: "6/04/2020"
output: html_document
---
## About the Dataset
This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```
### Load libraries
```{r, message=FALSE}

library(tidyverse)
library(GGally)
library(caret)
library(moments)
library(corrplot)
library(ggmap)
library(arules)
library(xgboost)

```
# Data import and Preparation

Import and examine datatype of each features
```{r}
dataset_path <- "/Users/truongminh/Downloads/SchoolWork/UW_Data_Analytics/Data_Mining/Final_Project"

houses <- read.delim(file.path(dataset_path,"kc_house_data.csv"),sep = ",", header = TRUE)

houses %>% glimpse()

```

```{r, echo = FALSE}
# Convert data type and construct new features
houses <- houses %>% mutate( date = date %>% substring(1,4) %>% as.integer(), #substring out the year
                             renovated = if_else(yr_renovated > 0, 1, 0) %>% as.factor(),
                             age = date - yr_built,
                             waterfront = waterfront %>% as.factor())


# Drop unnecessary features
houses <- houses %>% select(-c(id,date, yr_renovated, yr_built, zipcode))

```
# Exploratory Data Analysis

## Examine the shape of numeric variables
We will first examine the distribution of each numeric columns using histogram
```{r, echo = FALSE}

# Create a function to visualize the skewness of all numeric columns
plot.hists <- function(col, df, bins = 30){
  p1 <- ggplot(df, aes_string(col)) + 
    geom_histogram(aes(y = ..density..), bins = bins, 
                   alpha = 0.3, color = 'blue') +
    geom_density(size = 1) +
    labs(title=str_c("Histogram and density function \n for ", col))
  
  # Print the plot
  p1 %>% print()
}

# take all the numeric columns
num_cols <- houses %>% select(-c(lat, long ,waterfront,renovated)) %>% names()
walk(num_cols,plot.hists,houses)
```
From these histogram we can see that very few numeric features are normaly distributed which will decrease the reliability of the model given that the model use a linear algorithmn.

## Checking for correlation among variables with the target variable using correlation matrix
```{r}

corr_matrix <- houses %>% select(-c(lat,long,renovated,waterfront)) %>% cor()
corr_matrix %>% corrplot(method = "pie")
```

From this correlation map of the numeric features, we can see that:
- features relating to sizes seems to influence the prices of the house the most, namely: "bedrooms", "badrooms", "sqft_living", "sqft_lot", "sqft_above", "sqft_living15", "grade". However, we need to keep in mind that these features are also correlated with one another as well.Therefore, some of these features need to be filtered out to avoid multicolinearity.

- Unexpectedly, age has very little to contritute to the price of the residence
```{r}
#Feature extraction by filtering 
houses <- houses %>% select(-c(sqft_living15, sqft_lot15, sqft_above,age))
```
## Further explore the relation ship between size features and price
Let's furture explore the 3 discrete variables floors, bedrooms and bathrooms
```{r, echo = FALSE}

houses %>% ggplot(aes(x=as.factor(floors), y= price, fill = as.factor(floors))) + 
  geom_boxplot() +
  xlab("floors")

houses %>% ggplot(aes(x=as.factor(bedrooms), y= price, fill = as.factor(bedrooms))) + 
  geom_boxplot() +
  xlab("bedrooms")

houses %>% ggplot(aes(x=as.factor(bathrooms), y= price, fill = as.factor(bathrooms))) + 
  geom_boxplot() +
  xlab("bathrooms")

```
It seems that among the 3 variables, the number of bathrooms seems to be the feature that make the most significant impact to the price of the house

## Visualizng price by categorical variables 

Seeing how price differs in each variable grouping using viloin plot
```{r,echo = FALSE}

houses %>% ggplot(aes(x=renovated, y= price, fill = renovated)) + geom_violin()
houses %>% ggplot(aes(x=waterfront, y= price, fill = waterfront)) + geom_violin()
houses %>% ggplot(aes(x=as.factor(view), y= price, fill = as.factor(view))) + 
          geom_violin() + xlab("view")
houses %>% ggplot(aes(x=as.factor(grade), y= price, fill = as.factor(grade))) + 
          geom_violin() + xlab("grade")
```
 
 
 Each of these features has a visible effect on price. View seems to be the features where the effect on price is less distinctiive

## Visualizing price by location

Intuitively, it makes sense that the location of the house largely influence the price as well, let's visualize the distribution of house prices on map using longtitude and latitude as metrics

```{r, echo = FALSE, message=FALSE}

qmplot(long,lat, data = houses, geom = "blank") + 
   stat_density_2d(aes(color = price), geom = "polygon", alpha = 0.27, color = "#3ECAE8" )
 
```

We can see there is a pattern in how the higher priced houses are distributed. there is a high concentration of high price houses in the Ballard area, followed by West Seattle, Downtown Seattle, Mercer Island and Bellvue. Another interesting factor we can see from there is that a lot of the darker area are around waterfront housings

### Discretizing geoprahical values

It's clear how longtitude and lattitude influence house price. However, the current format of these variables make it difficult to extract meaningful information. Therefore, I'm gonna discretize tand group them into categories

```{r}

houses$vert_loc <- discretize(houses$lat,
                                    method = "frequency",
                                    breaks = 3,
                                    labels = c("South","Mid","North"))

houses$horz_loc <- discretize(houses$long,
                                     method = "frequency",
                                     breaks = 3,
                                     labels = c("West","Mid","East"))

houses <- houses %>% select(-c(lat,long))

```
# Preprocessing

## Transforming numerical features

From examining the histogram in the previous section, we came to the conclusion that most of the numeric features are highly skewed and does not follow a normal distribution. Let's check for their skewness

```{r}
num_feat = houses %>% select(-c(waterfront,renovated, vert_loc, horz_loc)) %>% names()
apply(houses[num_feat],2,skewness) # table showing the skewness of each variable
```
There are 2 highly skewed variables in this dataset: price and sqft_lot. To improve prediction accuracy, let's change all of them to log scale so that they will fit the Gausian Distribution

```{r}
# Convert skewed data to log scale
houses <- houses %>% mutate(sqft_lot = log10(sqft_lot),
                    price = log10(price))

```
## Encoding categorical features

```{r}

horz_loc <- model.matrix(~horz_loc - 1, data = houses)
vert_loc <- model.matrix(~vert_loc - 1, data = houses)
waterfront <- model.matrix(~waterfront - 1, data = houses)

houses <- cbind(houses %>% select(-c(horz_loc, vert_loc, waterfront)), horz_loc, vert_loc, waterfront)

```
## Split and Scale data
```{r}

train <- createDataPartition(y = houses$price, p = 0.7, list = FALSE)

houses_train <- houses[train,]
houses_test <- houses[-train,]

prepro <- preProcess(houses_train[num_feat],method = c("nzv","center","scale"))

houses_train <- predict(prepro,houses_train)
houses_test <- predict(prepro,houses_test)
```
# Modeling

For this regression problem, I'm going to try using 2 method namely ridge regression and xgboost regression tree. 
- Ridge is a regularized regression method which is known for its simplicity, it penaltize the model adjusted with the bias-variance trade off which mean it has lower accuracy on training data which make it much less likely to overfit. This penalty will also allows it to has less variance on unknown data

- XGBoost is a decision tree technique that is more complex, takes a little bit longer time to compute but returns better accuracy result, faster prediction. It also provides a lot of flexibility as it can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.However, like most tree method, XGBoost is likely to overfit but it's the least likely one in all of tree technique.
```{r}

set.seed(1808)
ridge_model <- train(price ~ .,
                     data = houses_train,
                     method = "ridge",
                     tuneGrid = expand.grid(lambda = seq(0, 1, 0.05)),
                     trControl = trainControl(method = "cv", 10))

xgboost_model <- train(price ~ .,
                      data = houses_train,
                      method = "xgbTree",
                      trControl = trainControl("cv", number = 10))

```
# Model evaluation

## Validating

Compare MAE, RMSE and Rsquared score after cross validation of each model

```{r}
compare <- resamples(list(xg_boost = xgboost_model,
                          ridge_regression = ridge_model))

bwplot(compare)
```
 
 Cross valiation score on training data shows xg_boost model yielding slighty better accuracy. XGBoost has always been known for its great accuracy, the big drawback of it is its possibility of overfitting, that's the main reason I decide to use ridge regression as my back up model as its compensate for these shortcoming of XGBoost in return for lower accuracy. 

 Let's see how each model perform on the training dataset

## Testing

```{r}
compare_on_train <- resamples(list(ridge = ridge_model,
                                   xgboost = xgboost_model))

compare_on_test <- data.frame(row.names = c('RMSE','RSquared','MAE'))

compare_on_test['ridge_model'] <- postResample(predict(ridge_model,houses_test),houses_test$price)
compare_on_test['xgboost_model'] <- postResample(predict(xgboost_model,houses_test),houses_test$price)

bwplot(compare_on_train)
compare_on_test
```

XGBoost still outperform the ridge model on unforseen data which once again prove its effectiveness. After examining their performance on the new training data, it's safe to use the XGBoost model now.

# Conclusion

In conclusion, with R-Squared at 0.8 the XGBoost model seems to be a good choice for predicting house prices. However, it worth mentioning that the scope of the variable used for these models are much larger than the variables given in the problem.

```{r}

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
